{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15Dd5aCKEr6DqX-S9ZQt1qWdqJ-7xHjXG",
      "authorship_tag": "ABX9TyP+XqSK8FaDC/i5J88aRE0d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prabhakars367/handwritten_text_recogn./blob/main/DOC_Q_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efDbsLeIFUKr",
        "outputId": "d8f77684-2eda-482e-ef74-193228a20482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttnzAZmhE5sX",
        "outputId": "912e42ca-fc14-4253-9737-5d0fb103f492"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForQuestionAnswering were not initialized from the model checkpoint at gpt2 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nVE2oVMdQ13n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe2cdfdf-5fec-4fc7-f020-5abff29b81f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install PyMuPDF\n",
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zS9htjiJeWL",
        "outputId": "77812347-2e2f-4979-92db-183aa7489e17"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.5 in /usr/local/lib/python3.10/dist-packages (from PyMuPDF) (1.23.5)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import fitz  # PyMuPDF\n",
        "import docx\n",
        "import os\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    text = \"\"\n",
        "\n",
        "    if file_path.lower().endswith('.pdf'):\n",
        "        with open(file_path, 'rb') as pdf:\n",
        "            pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "            for page_num in range(len(pdf_reader.pages)):\n",
        "                page = pdf_reader.pages[page_num]\n",
        "                text += page.extract_text()\n",
        "\n",
        "    elif file_path.lower().endswith('.xps') or file_path.lower().endswith('.oxps') or file_path.lower().endswith('.cbz') or file_path.lower().endswith('.cbr') or file_path.lower().endswith('.cb7'):\n",
        "        pdf_document = fitz.open(file_path)\n",
        "        for page_num in range(len(pdf_document)):\n",
        "            page = pdf_document.load_page(page_num)\n",
        "            text += page.get_text(\"text\")\n",
        "\n",
        "    elif file_path.lower().endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as txt:\n",
        "            text = txt.read()\n",
        "\n",
        "    elif file_path.lower().endswith('.docx'):\n",
        "        doc = docx.Document(file_path)\n",
        "        for paragraph in doc.paragraphs:\n",
        "            text += paragraph.text + '\\n'\n",
        "\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_path}\")\n",
        "\n",
        "    return text\n",
        "\n",
        "# Path for input\n",
        "input_file_path = \"/content/story1.txt\"\n",
        "\n",
        "document_text = extract_text_from_file(input_file_path)\n",
        "\n",
        "# output path\n",
        "output_file_path = \"/content/output.txt\"\n",
        "\n",
        "\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    output_file.write(document_text)\n",
        "\n",
        "print(f\"Text extracted from {input_file_path} and saved to {output_file_path}.\")\n"
      ],
      "metadata": {
        "id": "Ea771_1kRki9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b088ec91-ce6c-4383-a9ef-41c95322bab4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text extracted from /content/story1.txt and saved to /content/output.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install tesseract-ocr\n",
        "!apt-get install libtesseract-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjSmEwiVXjZW",
        "outputId": "4a602f1f-0ede-4d7d-beb3-a6534adb955e"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libtesseract-dev is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5bj-FVhz7wf",
        "outputId": "dd89cbc1-f1a7-44db-d596-abfe96e3d0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "import cv2\n",
        "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
        "image_path = '/content/hand_img.jpeg'\n",
        "img = cv2.imread(image_path)\n",
        "text = pytesseract.image_to_string(img)\n",
        "print(\"Recognized Text:\")\n",
        "print(text)\n",
        "document_text=text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8XRUBCBXAN9",
        "outputId": "0d54e09d-b457-4599-fdb0-d4c9ab1de800"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized Text:\n",
            "LAST Wedneschy\n",
            "We had » good\n",
            "Team building\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTsgcMLKzjo1",
        "outputId": "dc48ba7e-2745-446d-bc53-1e04c54f2481"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "def read_text(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(file_path, 'r', encoding='latin-1') as file:\n",
        "            text = file.read()\n",
        "    return text\n",
        "\n",
        "\n",
        "def sanitize_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    word_freq = {}\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            if word not in stop_words:\n",
        "                if word not in word_freq:\n",
        "                    word_freq[word] = 1\n",
        "                else:\n",
        "                    word_freq[word] += 1\n",
        "\n",
        "    max_freq = max(word_freq.values())\n",
        "    for word in word_freq:\n",
        "        word_freq[word] = (word_freq[word] / max_freq)\n",
        "\n",
        "    return word_freq, sentences\n",
        "\n",
        "def calculate_sentence_scores(sentences, word_freq):\n",
        "    sentence_scores = {}\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in nltk.word_tokenize(sentence):\n",
        "            if word in word_freq:\n",
        "                if sentence not in sentence_scores:\n",
        "                    sentence_scores[sentence] = word_freq[word]\n",
        "                else:\n",
        "                    sentence_scores[sentence] += word_freq[word]\n",
        "\n",
        "    return sentence_scores\n",
        "\n",
        "def generate_summary(file_path, num_sentences=5):\n",
        "    text = read_text(file_path)\n",
        "    word_freq, sentences = sanitize_text(text)\n",
        "    sentence_scores = calculate_sentence_scores(sentences, word_freq)\n",
        "\n",
        "    ranked_sentences = sorted(\n",
        "        ((sentence, score) for sentence, score in sentence_scores.items()),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    summary_sentences = [sentence for sentence, score in ranked_sentences[:num_sentences]]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "# Replace\n",
        "input_file = '/content/output.txt'\n",
        "num_sentences = 5\n",
        "\n",
        "summary = generate_summary(input_file, num_sentences)\n",
        "print(\"Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iX1DuYTz2vZ",
        "outputId": "4f4c78d9-32ac-471a-c407-ded2b8dc0213"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "From then on, John made sure to keep his spare key in a safe place, so he wouldn’t have to worry about being locked out again. He went to his neighbor’s house and asked if he could borrow the key. He had lost the key to his house and he couldn’t get inside. Suddenly, he remembered that he had given a spare key to his neighbor. His neighbor happily gave him the key and John was able to get inside his house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document = document_text\n",
        "question = \"What happened last wedneschy?\""
      ],
      "metadata": {
        "id": "NJGlxl4AN8xE"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(question, document, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "0uQ_ZYpoOE51"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(question, document, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "start_positions, end_positions = model(**inputs).values()\n",
        "\n",
        "answer_start = torch.argmax(start_positions)\n",
        "answer_end = torch.argmax(end_positions) + 1\n",
        "answer = tokenizer.decode(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJArN6MJOLFo",
        "outputId": "ddc510a9-e9d7-4c0a-c451-10085cb1e0ce"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What happened last wedneschy?\n",
            "Answer: What happened last\n"
          ]
        }
      ]
    }
  ]
}